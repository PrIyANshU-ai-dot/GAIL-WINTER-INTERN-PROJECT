# -*- coding: utf-8 -*-
"""gail4.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1slVt_-wG9RyN5LqQEer8LxL8oyNVQmvC
"""

# Import required libraries
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_absolute_error, r2_score
import matplotlib.pyplot as plt
import seaborn as sns
from scipy.optimize import minimize
from xgboost import XGBRegressor
from sklearn.preprocessing import PolynomialFeatures, StandardScaler
from google.colab import files

!pip install optuna

import optuna  
import shap  


uploaded = files.upload()
data = pd.read_csv("emissions_high_granularity.csv")

data["temperature"] = np.random.uniform(15, 35, size=len(data))  # Simulated weather data
data["process_efficiency"] = np.random.uniform(0.8, 1.0, size=len(data))  # Efficiency metric
data["equipment_age"] = np.random.randint(1, 20, size=len(data))  # Equipment aging
data["renewable_energy_share"] = np.random.uniform(0, 50, size=len(data))  # % renewable integration

features = [
    "production_value",
    "product_emissions_MtCO2",
    "flaring_emissions_MtCO2",
    "venting_emissions_MtCO2",
    "own_fuel_use_emissions_MtCO2",
    "fugitive_methane_emissions_MtCO2e",
    "temperature",
    "process_efficiency",
    "equipment_age",
    "renewable_energy_share",
]
target = "total_emissions_MtCO2e"

poly = PolynomialFeatures(degree=2, interaction_only=False, include_bias=False)
X_poly = poly.fit_transform(data[features])
feature_names_poly = poly.get_feature_names_out(features)

X_train, X_test, y_train, y_test = train_test_split(X_poly, data[target], test_size=0.2, random_state=42)

scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

def objective(trial):
    params = {
        "n_estimators": trial.suggest_int("n_estimators", 50, 300),
        "max_depth": trial.suggest_int("max_depth", 5, 20),
        "min_samples_split": trial.suggest_int("min_samples_split", 2, 20),
        "min_samples_leaf": trial.suggest_int("min_samples_leaf", 1, 10),
    }
    model = RandomForestRegressor(random_state=42, **params)
    model.fit(X_train_scaled, y_train)
    y_pred = model.predict(X_test_scaled)
    return mean_absolute_error(y_test, y_pred)

study = optuna.create_study(direction="minimize")
study.optimize(objective, n_trials=30)
best_params = study.best_params

model = RandomForestRegressor(random_state=42, **best_params)
model.fit(X_train_scaled, y_train)

y_pred = model.predict(X_test_scaled)
mae = mean_absolute_error(y_test, y_pred)
r2 = r2_score(y_test, y_pred)

print(f"Optimized Random Forest - MAE: {mae}, RÂ² Score: {r2}")

explainer = shap.TreeExplainer(model)
shap_values = explainer.shap_values(X_test_scaled)

shap.summary_plot(shap_values[1] if isinstance(shap_values, list) else shap_values, X_test_scaled, feature_names=feature_names_poly)

def weighted_objective_function(inputs):
    inputs_df = pd.DataFrame([inputs], columns=features)  # Create a DataFrame for the original features
    inputs_poly = poly.transform(inputs_df)  # Apply polynomial transformation
    inputs_scaled = scaler.transform(inputs_poly)  # Ensure inputs are scaled

    predicted_emissions = model.predict(inputs_scaled)[0]  # Predict using the model

    weights = [0.2, 0.3, 0.15, 0.15, 0.1, 0.1, 0.05, 0.05, 0.05, 0.05] + [0] * (len(inputs_poly[0]) - len(features))
    weighted_emissions = sum(w * x for w, x in zip(weights, inputs_poly[0]))  # Apply weights to polynomial features

    return predicted_emissions + weighted_emissions

bounds = [
    (0.9 * data["production_value"].mean(), 1.1 * data["production_value"].mean()),
    (0, data["product_emissions_MtCO2"].max()),
    (0, data["flaring_emissions_MtCO2"].max()),
    (0, data["venting_emissions_MtCO2"].max()),
    (0, data["own_fuel_use_emissions_MtCO2"].max()),
    (0, data["fugitive_methane_emissions_MtCO2e"].max()),
    (15, 35),  # Temperature bounds
    (0.8, 1.0),  # Process efficiency bounds
    (1, 20),  # Equipment age bounds
    (0, 50),  # Renewable energy share bounds
]

def constraint_production(inputs):
    min_production = 0.9 * data["production_value"].mean()
    return inputs[0] - min_production

constraints = [
    {"type": "ineq", "fun": constraint_production},
]

initial_guess = data[features].mean().values  # Use the mean values of the original features
result = minimize(
    weighted_objective_function,
    initial_guess,
    method="SLSQP",
    bounds=bounds,
    constraints=constraints,
)

optimized_inputs = result.x
optimized_emissions = result.fun

print("\nOptimized Inputs:")
for feature, value in zip(features, optimized_inputs):
    print(f"{feature}: {value:.2f}")
print(f"Optimized Emissions: {optimized_emissions:.2f} MtCO2e")


scenarios = {
    "10% Reduction in Venting": optimized_inputs.copy(),
    "15% Reduction in Flaring": optimized_inputs.copy(),
}
scenarios["10% Reduction in Venting"][3] *= 0.9  # Reduce venting emissions by 10%
scenarios["15% Reduction in Flaring"][2] *= 0.85  # Reduce flaring emissions by 15%

for scenario, inputs in scenarios.items():
    
    inputs_df = pd.DataFrame([inputs], columns=features)
    inputs_poly = poly.transform(inputs_df)
    inputs_scaled = scaler.transform(inputs_poly)

    
    emissions = model.predict(inputs_scaled)[0]
    print(f"{scenario}: {emissions:.2f} MtCO2e")

import joblib  # For saving models as .pkl files

joblib.dump(model, "random_forest_model.pkl")

joblib.dump(scaler, "scaler.pkl")

joblib.dump(poly, "polynomial_features.pkl")